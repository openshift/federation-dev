# Deploying MongoDB

The files within the [directory](../automated-demo/yaml-resources/mongo) are used with the KubeFed Controller to show
MongoDB running on multiple OpenShift clusters.

## Architecture

Shown below is the architecture definition for our MongoDB Cluster.

![MongoDB Cluster Architecture](./assets/federated-mongo-arch.png)

* There is a MongoDB pod running on each OpenShift Cluster, each pod has its own storage (provisioned by a block type StorageClass)
* The MongoDb pods are configured as a MongoDb ReplicaSet and communicate using TLS
* The OCP routes are configured as Passthrough, we need the connection to remain plain TLS (no HTTP headers involved)
* The MongoDB pods interact with each other using the OCP Routes (the nodes where these pods are running must be able to connect to the OCP routes)
* The apps consuming the MongoDB ReplicaSet have to use the proper MongoDB connection string URI

**How is the MongoDB ReplicaSet configured?**

Each OpenShift cluster has a MongoDB _pod_ running. There is a _service_ which routes the traffic received on por 27017/TCP to the MongoDB pod port 27017/TCP.

There is an _OpenShift Route_ created on each cluster, the _route_ has been configured to be passthrough (the HAProxy Router won't add HTTP headers to the connections). Each _route_ listens on port 443/TCP and reverse proxies traffic received to the mongo _service_ 27017/TCP.

The MongoDB pods have been configured to use TLS, so all connections will be made using TLS (a TLS certificate with routes and services DNS names configured as SANs is used by MongoDB pods).

Once the three pods are up and running, the MongoDB ReplicaSet is configured using the route hostnames as MongoDB endpoints. e.g:

Primary Replica: mongo-cluster1.apps.cluster1.example.com:443<br>
Secondary Replicas: mongo-cluster2.apps.cluster2.example.com:443, mongo-cluster3.apps.cluster3.example.com:443

In case a MongoDB pod fails / is stopped, the ReplicaSet will reconfigure itself, and once the failed / stopped pod is back, the ReplicaSet will include it again. (MongoDB Quorum Algorithm will decide if the ReplicaSet is RO or RW based on the quorum).

**NOTE:** This configuration doesn't require the different cluster networks to be aware of each other. We could get the same functionality using:

A) LoadBalancer Services (pods will use the service ip instead of the OpenShift Route)
B) Site2Site VPN like solution (pods will connect to ClusterIP / NodePort services directly)

## Prerequisites

### Creating Certificates
This demonstration uses MongoDB with TLS enabled. The example below will create a
generic CA, key, and certificate. 

Follow the steps below to create the required files in the `mongo-yaml` directory:

1. Change directory to `mongo-yaml`

    ~~~sh
    cd ~/federation-dev/labs/mongo-yaml
    ~~~

2. Create the file `ca-config.json`:

    ~~~sh
    cat > ca-config.json <<EOF
    {
      "signing": {
        "default": {
          "expiry": "8760h"
        },
        "profiles": {
          "kubernetes": {
            "usages": ["signing", "key encipherment", "server auth", "client auth"],
            "expiry": "8760h"
          }
        }
      }
    }
    EOF
    ~~~
3. Create the file `ca-csr.json`

    ~~~sh
    cat > ca-csr.json <<EOF
    {
      "CN": "Kubernetes",
      "key": {
        "algo": "rsa",
        "size": 2048
      },
      "names": [
        {
          "C": "US",
          "L": "Austin",
          "O": "Kubernetes",
          "OU": "TX",
          "ST": "Texas"
        }
      ]
    }
    EOF
    ~~~
4. Create the file `mongodb-csr.json`

    ~~~sh
    cat > mongodb-csr.json <<EOF
    {
      "CN": "kubernetes",
      "key": {
        "algo": "rsa",
        "size": 2048
      },
      "names": [
        {
          "C": "US",
          "L": "Austin",
          "O": "Kubernetes",
          "OU": "TX",
          "ST": "Texas"
        }
      ]
    }
    EOF
    ~~~

We will use OpenShift Routes to provide connectivity between MongoDB Replicas. As we said before, MongoDB will be configured to use TLS communications, we need to generate certificates with proper hostnames configured within them.

Follow the instructions below to generate a PEM file which include:
* MongoDB's Certificate Private Key
* MongoDB's Certificate Public Key

1. Generate the CA

    ~~~sh
    cfssl gencert -initca ca-csr.json | cfssljson -bare ca
    ~~~
2. Export some variables with information that will be used for generate the certificates

    ~~~sh
    NAMESPACE=mongo
    SERVICE_NAME=mongo
    ROUTE_CLUSTER1=mongo-cluster1.$(oc --context=cluster1 -n openshift-console get route console -o jsonpath='{.status.ingress[*].host}' | sed "s/.*\(apps.*\)/\1/g")
    ROUTE_CLUSTER2=mongo-cluster2.$(oc --context=cluster2 -n openshift-console get route console -o jsonpath='{.status.ingress[*].host}' | sed "s/.*\(apps.*\)/\1/g")
    ROUTE_CLUSTER3=mongo-cluster3.$(oc --context=cluster3 -n openshift-console get route console -o jsonpath='{.status.ingress[*].host}' | sed "s/.*\(apps.*\)/\1/g")
    SANS="localhost,localhost.localdomain,127.0.0.1,${ROUTE_CLUSTER1},${ROUTE_CLUSTER2},${ROUTE_CLUSTER3},${SERVICE_NAME},${SERVICE_NAME}.${NAMESPACE},${SERVICE_NAME}.${NAMESPACE}.svc.cluster.local"
    ~~~
3. Generate the MongoDB Certificates

    ~~~sh
    cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -hostname=${SANS} -profile=kubernetes mongodb-csr.json | cfssljson -bare mongodb
    ~~~
4. Combine Key and Certificate

    ~~~sh
    cat mongodb-key.pem mongodb.pem > mongo.pem
    ~~~

### Update MongoDB yaml files

The lab content provides the required files for deploying the federated MongoDB ReplicaSet in YAML format (similar to the previous lab).

Before deploying MongoDB the yaml files need to be updated to define the certificates that
were created, routing endpoints that will be used and cluster names that will be used for 
KubeFed placement policies.

1. Configure `MongoDB's PEM`

    ~~~sh
    sed -i "s/mongodb.pem: .*$/mongodb.pem: $(openssl base64 -A < mongo.pem)/" 01-mongo-federated-secret.yaml
    sed -i "s/ca.pem: .*$/ca.pem: $(openssl base64 -A < ca.pem)/" 01-mongo-federated-secret.yaml
    ~~~
2. Configure `MongoDB's Endpoints`

    ~~~sh
    sed -i "s/primarynodehere/${ROUTE_CLUSTER1}:443/" 04-mongo-federated-deployment-rs.yaml
    sed -i "s/replicamembershere/${ROUTE_CLUSTER1}:443,${ROUTE_CLUSTER2}:443,${ROUTE_CLUSTER3}:443/" 04-mongo-federated-deployment-rs.yaml
    ~~~

## Deploying the MongoDB Cluster

There are many different types of federated objects but they are somewhat similar to those
non-federated objects. For more information about federated objects see the following  [examples](https://github.com/kubernetes-sigs/kubefed/tree/master/example/sample1) and
the [user guide](https://github.com/kubernetes-sigs/kubefed/blob/master/docs/userguide.md).

The mongo namespace must be created and then defined as a federated namespace.
~~~sh
oc --context=cluster1 create ns mongo
kubefedctl federate namespace mongo --host-cluster-context cluster1
~~~

Validate the namespace exists in the three clusters.
~~~sh
for i in cluster1 cluster2 cluster3; do oc get namespace mongo --context $i; done

NAME    STATUS   AGE
mongo   Active   6s
NAME    STATUS   AGE
mongo   Active   5s
NAME    STATUS   AGE
mongo   Active   5s
~~~

Now that the yaml files contain the `pem` and routes it is time to deploy the objects.

1. Create two FederatedSecrets, one with MongoDB Admin Credentials and one with TLS Certificates information

    ~~~sh
    oc --context=cluster1 -n mongo create -f 01-mongo-federated-secret.yaml
    ~~~
2. Create the FederatedService that will create a `ClusterIP Service` for MongoDB

    ~~~sh
    oc --context=cluster1 -n mongo create -f 02-mongo-federated-service.yaml
    ~~~
3. Create the FederatedPersistentVolumeClaim that will create a `storage claim` to be used by MongoDB

    ~~~sh
    oc --context=cluster1 -n mongo create -f 03-mongo-federated-pvc.yaml
    ~~~
4. Create the FederatedDeployment that will create a `Deployment` which deploys MongoDB

    ~~~sh
    oc --context=cluster1 -n mongo create -f 04-mongo-federated-deployment-rs.yaml
    ~~~

Next step in the MongoDB deployment consists of creating `OpenShift Routes` that will be used as the 
public endpoints for the MongoDB deployment. We will create one Route per cluster.

> **NOTE:** In the previous lab we created the routes using `oc expose`, for this lab we need to use more advanced options, so we will be using `oc create route`

~~~sh
# Cluster1 Route
oc --context=cluster1 -n mongo create route passthrough mongo --service=mongo --port=27017 --hostname=${ROUTE_CLUSTER1}
# Cluster2 Route
oc --context=cluster2 -n mongo create route passthrough mongo --service=mongo --port=27017 --hostname=${ROUTE_CLUSTER2}
# Cluster3 Route
oc --context=cluster3 -n mongo create route passthrough mongo --service=mongo --port=27017 --hostname=${ROUTE_CLUSTER3}
~~~

Verify `OpenShift Routes` creation.

~~~sh
for cluster in cluster1 cluster2 cluster3; do oc --context=$cluster -n mongo get route mongo; done
~~~

## Configuring MongoDB ReplicaSet

At this point we should have 3 independent MongoDB instances running, one on each cluster. Let's verify all replicas are up and running. 

> **NOTE:** This may take a minute or two

~~~sh
for cluster in cluster1 cluster2 cluster3; do wait-for-deployment $cluster mongo mongo; done

Checking if deployment mongo from namespace mongo on cluster cluster1 is ready
<OUTPUT_OMITTED>
Deployment is ready
Checking if deployment mongo from namespace mongo on cluster cluster2 is ready
<OUTPUT_OMITTED>
Deployment is ready
Checking if deployment mongo from namespace mongo on cluster cluster3 is ready
<OUTPUT_OMITTED>
Deployment is ready
~~~

Now that all replicas are up and running we are going to configure a MongoDB ReplicaSet so all three replicas work as a cluster.
This procedure has been automated and you only need to add a label to the MongoDB Pod you want to act as primary replica. We are going to use the pod running on `cluster1` as primary replica.

~~~sh
# Select Primary MongoDB pod
MONGO_POD=$(oc --context=cluster1 -n mongo get pod --selector="name=mongo" --output=jsonpath='{.items..metadata.name}')
# Label primary pod
oc --context=cluster1 -n mongo label pod $MONGO_POD replicaset=primary
~~~

The MongoDB ReplicaSet is being configured now, let's wait for it to be configured and check the ReplicaSet Status to ensure it has been properly configured.

~~~sh
wait-for-mongo-replicaset cluster1 mongo 3

Checking if MongoDB Replicaset from namespace mongo on cluster cluster1 is configured
<OUTPUT_OMITTED>

MongoDB ReplicaSet Status:
--------------------------
<OUTPUT_OMITTED>
~~~

If you want to get a more detailed view of the configuration, you can run the following command and you will get a huge json output with the status of the ReplicaSet:

~~~sh
# Select Primary MongoDB pod
MONGO_POD=$(oc --context=cluster1 -n mongo get pod --selector="name=mongo" --output=jsonpath='{.items..metadata.name}')
# Get replicaset status
oc --context=cluster1 -n mongo exec $MONGO_POD -- bash -c 'mongo --norc --quiet --username=admin --password=$MONGODB_ADMIN_PASSWORD --host localhost admin --tls --tlsCAFile /opt/mongo-ssl/ca.pem --eval "rs.status()"'
~~~

This concludes the deployment of MongoDB using KubeFed.

If a mistake was made, please let an instructor know, and review the [cleanup instructions](./cleanup-instructions.md).

Next Lab: [Lab 6 - Deploying Pacman](./6.md)<br>
Previous Lab: [Lab 4 - Example Application One](./4.md)<br>
[Home](./README.md)

