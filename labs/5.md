<a id="markdown-customizing-deployments" name="customizing-deployments"></a>
# Customizing deployments
Certain situations exist especially in multicluster scenarios and in development workflows where an application requires specific values to be unique for that environment.
*Kustomize* was created to handle that specific situation and as of Kubernetes 1.14 the functionality is integrated into `kubectl`.

<a id="markdown-creating-kustomized-apps" name="creating-kustomized-apps"></a>
## Creating a Kustomized Apps 
This lab is going to walk through the details of deploying a project and resources that use the same YAML files but are unique per cluster based on values provided by *Kustomize*.


The [lab-5-assets](./lab-5-assets/base) contains definitions to deploy these resources.

The assets will be loaded into Argo CD to be deployed and managed but each cluster will have unique values used by the configmap, deployment, and service.

1. Change directory to `lab-5-assets` to see the assets to be created.

    ~~~sh
    cd ~/federation-dev/labs/lab-5-assets
    ~~~
4. Create an application that uses the defined git repository and specify `--nameprefix dev`. This will specify that all items created should have the prefix of `dev` added to the object name. The namespace must be created first as using ArgoCD currently with namespace creation doesn't work.

    ~~~sh
    oc create ns dev-web-site
    argocd app create --project default --name cluster1-kustomize --repo http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git --path labs/lab-5-assets/base --dest-server $(argocd cluster list | grep cluster1 | awk '{print $1}')  --dest-namespace dev-web-site  --revision master --nameprefix dev- --sync-policy automated
    ~~~
5. List the newly defined app. An important thing to notice is that the application will have a status of *OutOfSync*

    ~~~sh
    argocd app list
    ~~~
6. The object should automatically be created as automated sync is enabled. We will run the app sync again just to verify a healthy status. This will provide a detail read out on the success of the deployment of objects. 

   ~~~sh
   argocd app sync cluster1-kustomize
   ~~~

   You should see a value of:
   
   * **Health Status:** `Healthy` 
   * **Message:** `successfully synced (all tasks run)`

<a id="markdown-verify-that-the-application-is-running" name="verify-that-the-application-is-running"></a>
## Verify that the application is running

Verify that the various resources have been deployed. The most important thing to note is that the name prefix of dev hs been assigned to the deployment, service, and configmap. 

~~~sh
for resource in deployments services pods; do
  echo ------------ cluster1 ${resource} ------------
  oc --context cluster1 -n dev-web-site get ${resource}
done
~~~

~~~sh
------------ cluster1 deployments ------------
NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
dev-the-deployment   1/1     1            1           5m21s
------------ cluster1 services ------------
NAME              TYPE           CLUSTER-IP       EXTERNAL-IP                                                              PORT(S)          AGE
dev-the-service   LoadBalancer   172.31.118.121   ab15ca20bc53711e9beca0243f00f22e-426184387.us-east-1.elb.amazonaws.com   8666:30409/TCP   5m22s
------------ cluster1 pods ------------
NAME                                  READY   STATUS    RESTARTS   AGE
dev-the-deployment-5fc896d994-w8zp4   1/1     Running   0          5m22s
~~~

The stength of using *Kustomize* with GitOps is that we can use the same exact YAML files but specify a different `--dest-namespace` and ` --nameprefix` arguments and deploy similar bits but with different parameters.

<a id="markdown-kustomize-overlays" name="kustomize-overlays"></a>
## Kustomize Overlays
*Kustomize* offers the ability to create `overlays`. These are directories with subdirectories below that contain specific changes.

We will use *Kustomize* to deploy the same resources in our 3 clusters but the configmap will be different per cluster. We will also specify different amounts of `replicas` to be deployed per cluster.

First, we will create a namespace in each cluster.

~~~sh
oc --context cluster1 create ns web-app
oc --context cluster2 create ns web-app
oc --context cluster3 create ns web-app
~~~

Within the directory [overlay-assets](./lab-5-assets/overlay-assets/overlays) directories for each of the 3 clusters. These directories contain a unique configmap and deployment file. These files will be used to define any customization.

For example, in cluster1 we will have 1 replica while cluster 3 will have 3 replicas. Below you will see the customizations for cluster1.

The ConfigMap will tell you the cluster in which the application is running.
~~~yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: the-map
data:
  altGreeting: "The app is running on cluster3"
~~~

The deployment file will override the amount of replicas in the original `deployment.yaml` with the value defined within the overlay directory.
~~~yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: the-deployment
spec:
  replicas: 3
~~~

If your organization also wanted to do canary deployments. The image could be changed within the *kustomize.yaml* file, an example can be found [here](https://github.com/kubernetes-sigs/kustomize/tree/master/examples/transformerconfigs#images-transformer).

Let's define the cluster1 application.

~~~sh
argocd app create --project default --name cluster1-web-app --repo http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git --path labs/lab-5-assets/overlay-assets/overlays/cluster1 --dest-server $(argocd cluster list | grep cluster1 | awk '{print $1}') --dest-namespace web-app --revision master --sync-policy automated
~~~

Verify the objects deployed correctly using the argocd binary.
~~~sh
argocd app get cluster1-web-app
~~~

Wait until the **Health Status** reports `Healthy`. Once it is healthy, we will define the two remaining clusters.

NOTE: The *--dest-server* parameter should refer to server value frrom argo cluster list.
~~~sh
argocd app create --project default --name cluster2-web-app --repo http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git --path labs/lab-5-assets/overlay-assets/overlays/cluster2 --dest-server $(argocd cluster list | grep cluster2 | awk '{print $1}') --dest-namespace web-app --revision master --sync-policy automated

argocd app create --project default --name cluster3-web-app --repo http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git --path labs/lab-5-assets/overlay-assets/overlays/cluster3 --dest-server $(argocd cluster list | grep cluster3 | awk '{print $1}') --dest-namespace web-app --revision master --sync-policy automated
~~~

Verify the objects deployed correctly using the argocd binary.
~~~sh
argocd app get cluster2-web-app
argocd app get cluster3-web-app
~~~

Wait until the **Health Status** reports `Healthy`. Once both clusters show `Healthy`, then verify the deployments have been created on all the clusters.
~~~sh
for cluster in cluster1 cluster2 cluster3; do
    echo ------------ ${cluster} deployments ------------
    oc --context ${cluster} -n web-app get deployments
done
~~~

Finally, we will look up the load balancer service and verify that the unique values that were specified by the configmap are shown when we curl the page.
~~~sh
for cluster in cluster1 cluster2 cluster3; do
  echo ------------ ${cluster} deployments ------------
  url=$(oc --context ${cluster} -n web-app get svc $cluster-the-service -o jsonpath='{.status.loadBalancer.ingress[*].hostname}')
  curl http://$url:8666
done
~~~

Next Lab: [Lab 6 - Deploying MongoDB](./6.md)<br>
Previous Lab: [Lab 4 - Deploying and Managing a Project with GitOps](./4.md)<br>
[Home](./README.md)
