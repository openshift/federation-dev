# Deploying MongoDB

The files within the [directory](./mongo-yaml) are used with the KubeFed Controller to show
MongoDB running on multiple OpenShift clusters.

## Prerequisites

### Creating Certificates
This demonstration uses MongoDB with TLS enabled. The example below will create a
generic CA, key, and certificate. The following binaries are required.

* [cssl](https://pkg.cfssl.org/R1.2/cfssl_linux-amd64)
* [cfssljson](https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64)

~~~sh
curl -LOs https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
mv cfssl_linux-amd64 ~/bin/cfssl
chmod +x ~/bin/cfssl
curl -LOs https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
mv cfssljson_linux-amd64 ~/bin/cfssljson
chmod +x ~/bin/cfssljson
~~~

Create the following files to be used for certificate signing. The `ca-csr.json` and `mongodb-csr.json` should reflect values that may be relevant to the organization or OpenShift clusters.

Follow the steps below to create the required files in the `mongo-yaml` directory:

1. Change directory to `mongo-yaml`

    ~~~sh
    cd federation-dev/labs/mongo-yaml
    ~~~

2. Edit the file `ca-config.json`:

    ~~~sh
    vim ca-config.json
    ~~~
    2.1 Place the content below inside the `ca-config.json` file

    ```json
    {
      "signing": {
        "default": {
          "expiry": "8760h"
        },
        "profiles": {
          "kubernetes": {
            "usages": ["signing", "key encipherment", "server auth", "client auth"],
            "expiry": "8760h"
          }
        }
      }
    }
    ```
3. Edit the file `ca-csr.json`

    ~~~sh
    vim ca-csr.json
    ~~~
    3.1 Place the content below inside the `ca-csr.json` file

    ```json
    {
      "CN": "Kubernetes",
      "key": {
        "algo": "rsa",
        "size": 2048
      },
      "names": [
        {
          "C": "US",
          "L": "Austin",
          "O": "Kubernetes",
          "OU": "TX",
          "ST": "Texas"
        }
      ]
    }
    ```
4. Edit the file `mongodb-csr.json`

    ~~~sh
    vim mongodb-csr.json
    ~~~
    3.1 Place the content below inside the `mongodb-csr.json` file

    ```json
    {
      "CN": "kubernetes",
      "key": {
        "algo": "rsa",
        "size": 2048
      },
      "names": [
        {
          "C": "US",
          "L": "Austin",
          "O": "Kubernetes",
          "OU": "TX",
          "ST": "Texas"
        }
      ]
    }
    ```

We will use OpenShift Routes to provide connectivity between MongoDB Replicas. As we said before, MongoDB will be configured to use TLS communications, we need to generate certificates with proper hostnames configured within them.

Follow the instructions below to generate a PEM file which include:
* MongoDB's Certificate Private Key
* MongoDB's Certificate Public Key

1. Export some variables with information that will be used for generate the certificates

    ~~~sh
    NAMESPACE=mongo
    SERVICE_NAME=mongo
    ROUTE_CLUSTER1=mongo-cluster1.$(oc --context=cluster1 -n openshift-console get route console -o jsonpath='{.status.ingress[*].host}' | sed "s/.*\(apps.*\)/\1/g")
    ROUTE_CLUSTER2=mongo-cluster2.$(oc --context=cluster2 -n openshift-console get route console -o jsonpath='{.status.ingress[*].host}' | sed "s/.*\(apps.*\)/\1/g")
    ROUTE_CLUSTER3=mongo-cluster3.$(oc --context=cluster3 -n openshift-console get route console -o jsonpath='{.status.ingress[*].host}' | sed "s/.*\(apps.*\)/\1/g")
    SANS="localhost,localhost.localdomain,127.0.0.1,${ROUTE_CLUSTER1},${ROUTE_CLUSTER2},${ROUTE_CLUSTER3},${SERVICE_NAME},${SERVICE_NAME}.${NAMESPACE},${SERVICE_NAME}.${NAMESPACE}.svc.cluster.local"
    ~~~
2. Generate the CA

    ~~~sh
    cfssl gencert -initca ca-csr.json | cfssljson -bare ca
    ~~~
3. Generate the MongoDB Certificates

    ~~~sh
    cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -hostname=${SANS} -profile=kubernetes mongodb-csr.json | cfssljson -bare mongodb
    ~~~
4. Combine Key and Certificate

    ~~~sh
    cat mongodb-key.pem mongodb.pem > mongo.pem
    ~~~

### Update MongoDB yaml files

Before deploying MongoDB the yaml files need to be updated to define the certificates that
were created, routing endpoints that will be used and cluster names that will be used for 
KubeFed placement policies.

1. Configure `MongoDB's PEM`

    ~~~sh
    sed -i "s/mongodb.pem: .*$/mongodb.pem: $(openssl base64 -A < mongo.pem)/" 01-mongo-federated-secret.yaml
    sed -i "s/ca.pem: .*$/ca.pem: $(openssl base64 -A < ca.pem)/" 01-mongo-federated-secret.yaml
    ~~~
2. Configure `MongoDB's Endpoints`

    ~~~sh
    sed -i "s/primarynodehere/${ROUTE_CLUSTER1}:443/" 04-mongo-federated-deployment-rs.yaml
    sed -i "s/replicamembershere/${ROUTE_CLUSTER1}:443,${ROUTE_CLUSTER2}:443,${ROUTE_CLUSTER3}:443/" 04-mongo-federated-deployment-rs.yaml
    ~~~
3. Update cluster names to reflect the names of the kubefedclusters defined in [Lab 3](./3.md#markdown-register-the-clusters)

    ~~~sh
    sed -i 's/feddemocl1/cluster1/g' ./*.yaml
    sed -i 's/feddemocl2/cluster2/g' ./*.yaml
    sed -i 's/feddemocl3/cluster3/g' ./*.yaml
    ~~~

## Deploying the MongoDB Cluster

There are many different types of federated objects but they are somewhat similar to those
non-federated objects. For more information about federated objects see the following  [examples](https://github.com/kubernetes-sigs/kubefed/tree/master/example/sample1) and
the [user guide](https://github.com/kubernetes-sigs/kubefed/blob/master/docs/userguide.md).

The mongo namespace must be created and then defined as a federated namespace.
~~~sh
oc create ns mongo
kubefedctl federate namespace mongo
~~~

Validate the namespace exists in the three clusters.
~~~sh
for i in cluster1 cluster2 cluster3; do oc get project mongo --context $i; done
NAME     DISPLAY NAME   STATUS
mongo                  Active
NAME     DISPLAY NAME   STATUS
mongo                  Active
NAME     DISPLAY NAME   STATUS
mongo                  Active
~~~

Now that the yaml files contain the `pem` and routes it is time to deploy the objects.

1. Create the FederatedSecret containing two `Secrets`, one with MongoDB Admin Credentials and one with TLS Certificates information

    ~~~sh
    oc --context=cluster1 -n mongo create -f 01-mongo-federated-secret.yaml
    ~~~
2. Create the FederatedService that will create a `ClusterIP Service` for MongoDB

    ~~~sh
    oc --context=cluster1 -n mongo create -f 02-mongo-federated-service.yaml
    ~~~
3. Create the FederatedPersistentVolumeClaim that will create a `storage claim` to be used by MongoDB

    ~~~sh
    oc --context=cluster1 -n mongo create -f 03-mongo-federated-pvc.yaml
    ~~~
4. Create the FederatedDeployment that will create a `Deployment` which deploys MongoDB

    ~~~sh
    oc --context=cluster1 -n mongo create -f 04-mongo-federated-deployment-rs.yaml
    ~~~

Next step in the MongoDB deployment consists of creating `OpenShift Routes` that will be used as the 
public endpoints for the MongoDB deployment. We will create one Route per cluster.

~~~sh
# Cluster1 Route
oc --context=cluster1 -n mongo create route passthrough mongo --service=mongo --port=27017 --hostname=${ROUTE_CLUSTER1}
# Cluster2 Route
oc --context=cluster2 -n mongo create route passthrough mongo --service=mongo --port=27017 --hostname=${ROUTE_CLUSTER2}
# Cluster3 Route
oc --context=cluster3 -n mongo create route passthrough mongo --service=mongo --port=27017 --hostname=${ROUTE_CLUSTER3}
~~~

## Configuring MongoDB ReplicaSet

At this point we should have 3 independent MongoDB instances running, one on each cluster. Let's verify all replicas are up and running.

~~~sh
# Wait until Cluster1 mongodb deployment is ready
oc --context=cluster1 -n mongo get deployment mongo
# Wait until Cluster2 mongodb deployment is ready
oc --context=cluster2 -n mongo get deployment mongo
# Wait until Cluster3 mongodb deployment is ready
oc --context=cluster3 -n mongo get deployment mongo
~~~

Now that all replicas are upd and running we are going to configure a MongoDB ReplicaSet so all three replicas work as a cluster.
This procedure has been automated and you only need to add a label to the MongoDB Pod you want to act as primary replica. We are going to use the pod running on `cluster1` as primary replica.

~~~sh
# Select Primary MongoDB pod
MONGO_POD=$(oc --context=cluster1 -n mongo get pod --selector="name=mongo" --output=jsonpath='{.items..metadata.name}')
# Label primary pod
oc --context=cluster1 -n mongo label pod $MONGO_POD replicaset=primary
~~~

The MongoDB ReplicaSet is being configured now, let's wait 30 seconds and check the ReplicaSet Status to ensure it has been properly configured.

~~~sh
# Wait 30 seconds so the MongoDB ReplicaSet is configured
sleep 30
# Get replicaset status
oc --context=cluster1 -n mongo exec $MONGO_POD -- bash -c 'mongo --norc --quiet --username=admin --password=$MONGODB_ADMIN_PASSWORD --host localhost admin --tls --tlsCAFile /opt/mongo-ssl/ca.pem --eval "rs.status()"'
~~~

Using the output below it is possible to identify the primary and secondary MongoDB
servers.

~~~json
"members" : [
  {
    "_id" : 0,
    "name" : "mongo-east1.apps.east-1.example1.com:443",
    "health" : 1,
    "state" : 1,
    "stateStr" : "PRIMARY",
    "uptime" : 112,
    "optime" : {
      "ts" : Timestamp(1556819163, 6),
      "t" : NumberLong(1)
    },
    "optimeDate" : ISODate("2019-05-02T17:46:03Z"),
    "syncingTo" : "",
    "syncSourceHost" : "",
    "syncSourceId" : -1,
    "infoMessage" : "could not find member to sync from",
    "electionTime" : Timestamp(1556819161, 1),
    "electionDate" : ISODate("2019-05-02T17:46:01Z"),
    "configVersion" : 1,
    "self" : true,
    "lastHeartbeatMessage" : ""
  },
  {
    "_id" : 1,
    "name" : "mongo-east2.apps.east-2.example1.com:443",
    "health" : 1,
    "state" : 2,
    "stateStr" : "SECONDARY",
    "uptime" : 30,
    "optime" : {
      "ts" : Timestamp(1556819163, 6),
      "t" : NumberLong(1)
    },
    "optimeDurable" : {
      "ts" : Timestamp(1556819163, 6),
      "t" : NumberLong(1)
    },
    "optimeDate" : ISODate("2019-05-02T17:46:03Z"),
    "optimeDurableDate" : ISODate("2019-05-02T17:46:03Z"),
    "lastHeartbeat" : ISODate("2019-05-02T17:46:19.469Z"),
    "lastHeartbeatRecv" : ISODate("2019-05-02T17:46:19.908Z"),
    "pingMs" : NumberLong(13),
    "lastHeartbeatMessage" : "",
    "syncingTo" : "mongo-east1.apps.east-1.example1.com:443",
    "syncSourceHost" : "mongo-east1.apps.east-1.example1.com:443",
    "syncSourceId" : 0,
    "infoMessage" : "",
    "configVersion" : 1
  },
  {
    "_id" : 2,
    "name" : "mongo-west2.apps.west-2.example1.com:443",
    "health" : 1,
    "state" : 2,
    "stateStr" : "SECONDARY",
    "uptime" : 30,
    "optime" : {
      "ts" : Timestamp(1556819163, 6),
      "t" : NumberLong(1)
    },
    "optimeDurable" : {
      "ts" : Timestamp(1556819163, 6),
      "t" : NumberLong(1)
    },
    "optimeDate" : ISODate("2019-05-02T17:46:03Z"),
    "optimeDurableDate" : ISODate("2019-05-02T17:46:03Z"),
    "lastHeartbeat" : ISODate("2019-05-02T17:46:20.730Z"),
    "lastHeartbeatRecv" : ISODate("2019-05-02T17:46:19.024Z"),
    "pingMs" : NumberLong(79),
    "lastHeartbeatMessage" : "",
    "syncingTo" : "mongo-east2.apps.east-2.example1.com:443",
    "syncSourceHost" : "mongo-east2.apps.east-2.example1.com:443",
    "syncSourceId" : 1,
    "infoMessage" : "",
    "configVersion" : 1
  }
],
~~~

This concludes the deployment of MongoDB using KubeFed.



Next Lab: [Lab 6 - Deploying Pacman](./6.md)<br>
Previous Lab: [Lab 4 - Example Application One](./4.md)<br>
[Home](../README.md)
