<a id="markdown-customizing-deployments" name="customizing-deployments"></a>
# Customizing deployments
Certain situations exist especially in multicluster scenarios and in development workflows where an application requires specific values to be unique for that environment.
*Kustomize* was created to handle that specific situation and as of Kubernetes 1.14 the functionality is integrated into `kubectl`.

<a id="markdown-creating-kustomized-apps" name="creating-kustomized-apps"></a>
## Creating a Kustomized Apps 
This lab is going to walk through the details of deploying a project and resources that use the same YAML files but are unique per cluster based on values provided by *Kustomize*.


The [lab-5-assets](./lab-5-assets/base) contains definitions to deploy these resources.

The assets will be loaded into Argo CD to be deployed and managed but each cluster will have unique values used by the configmap, deployment, and service.

1. Change directory to `lab-5-assets` to see the assets to be created.

    ~~~sh
    cd ~/federation-dev/labs/lab-5-assets
    ~~~

2. Modify the route hostname for the application

    ~~~sh
    ROUTE_CLUSTER1=kustomized-app.$(oc --context=cluster1 get ingresses.config.openshift.io cluster -o jsonpath='{ .spec.domain }')
    sed -i "s/changeme/${ROUTE_CLUSTER1}/" base/route.yaml
    ~~~
3. Commit the changes

Before we can commit the changes in the git repository, we must configure the git client with your name and email address. Otherwise, a warning message would be displayed.

Set the name and email address by using the following values or substitute with your own name and email address

    ~~~sh
    git config --global user.name "Student User"
    git config --global user.email student@redhat.com
    ~~~

Now, commit the changes and push the changes to the git repository

    ~~~sh
    git commit -am 'Route Hostname configured for Cluster1'
    git push origin master
    ~~~
4. Create an application that uses the defined git repository and specify `--nameprefix dev`. This will specify that all items created should have the prefix of `dev` added to the object name.

    ~~~sh
    argocd app create --project default --name cluster1-kustomize --repo http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git --path labs/lab-5-assets/base --dest-server $(argocd cluster list | grep cluster1 | awk '{print $1}')  --dest-namespace dev-web-site  --revision master --nameprefix dev- --sync-policy automated
    ~~~
5. List the newly defined app. If the sync has not been completed it is possible that the application will have a status of *OutOfSync*

    ~~~sh
    argocd app get cluster1-kustomize
    ~~~
6. The object should automatically be created as automated sync is enabled. We will run the app sync again just to verify a healthy status. This will provide a detail read out on the success of the deployment of objects. 

   ~~~sh
   argocd app sync cluster1-kustomize
   ~~~

   You should see a value of:
   
   * **Health Status:** `Healthy` 
   * **Message:** `successfully synced (all tasks run)`

<a id="markdown-verify-that-the-application-is-running" name="verify-that-the-application-is-running"></a>
## Verify that the application is running

Verify that the various resources have been deployed. The most important thing to note is that the name prefix of dev hs been assigned to the deployment, service, and configmap. 

~~~sh
# The for loop below will display objects in the dev-web-site namespace
for resource in deployments services pods route; do
  echo ------------ cluster1 ${resource} ------------
  oc --context cluster1 -n dev-web-site get ${resource}
done

------------ cluster1 deployments ------------
NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
dev-the-deployment   1/1     1            1           5m9s
------------ cluster1 services ------------
NAME              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
dev-the-service   ClusterIP   172.30.50.181   <none>        8080/TCP   5m10s
------------ cluster1 pods ------------
NAME                                  READY   STATUS    RESTARTS   AGE
dev-the-deployment-5fc896d994-sl4ft   1/1     Running   0          5m9s
------------ cluster1 route ------------
NAME            HOST/PORT                                                      PATH   SERVICES      PORT   TERMINATION   WILDCARD
dev-the-route   kustomized-app.apps.cluster1.example.com                              the-service   8080                 None
~~~

The strength of using *Kustomize* with GitOps is that we can use the same exact YAML files but specify a different `--dest-namespace` and `--nameprefix` arguments and deploy similar bits but with different parameters.

<a id="markdown-kustomize-overlays" name="kustomize-overlays"></a>
## Kustomize Overlays
*Kustomize* offers the ability to create [overlays](https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#overlay). These are directories with subdirectories below that contain specific changes.

We will use *Kustomize* to deploy the same resources in our 3 clusters but the configmap will be different per cluster. We will also specify different amounts of `replicas` to be deployed per cluster.

First, we need to modify the Route hostnames that will be used by our clusters.

~~~sh
# Define the variable of `ROUTE_CLUSTER1`
ROUTE_CLUSTER1=web-app.$(oc --context=cluster1 get ingresses.config.openshift.io cluster -o jsonpath='{ .spec.domain }')
# Define the variable of `ROUTE_CLUSTER2`
ROUTE_CLUSTER2=web-app.$(oc --context=cluster2 get ingresses.config.openshift.io cluster -o jsonpath='{ .spec.domain }')
# Define the variable of `ROUTE_CLUSTER3`
ROUTE_CLUSTER3=web-app.$(oc --context=cluster3 get ingresses.config.openshift.io cluster -o jsonpath='{ .spec.domain }')
# Replace the value of changme with `ROUTE_CLUSTER1` in the file `route.yaml`
sed -i "s/changeme/${ROUTE_CLUSTER1}/" overlay-assets/overlays/cluster1/route.yaml
# Replace the value of changme with `ROUTE_CLUSTER2` in the file `route.yaml`
sed -i "s/changeme/${ROUTE_CLUSTER2}/" overlay-assets/overlays/cluster2/route.yaml
# Replace the value of changme with `ROUTE_CLUSTER3` in the file `route.yaml`
sed -i "s/changeme/${ROUTE_CLUSTER3}/" overlay-assets/overlays/cluster3/route.yaml
~~~

We are going to commit our changes.

~~~sh
# Stage your changes to be sent to the git repository
git commit -am 'Route Hostname configured for all three clusters'
# Push your commits to the git repository
git push origin master
~~~

Within the directory [overlay-assets](./lab-5-assets/overlay-assets/overlays) directories for each of the 3 clusters. These directories contain a unique configmap, deployment and route file. These files will be used to define any customization.

For example, in cluster1 we will have 1 replica while cluster 3 will have 3 replicas. Below you will see the customizations for cluster3.

The ConfigMap will tell you the cluster in which the application is running.

~~~yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: the-map
data:
  altGreeting: "The app is running on cluster3"
~~~

The deployment file will override the amount of replicas in the original `deployment.yaml` with the value defined within the overlay directory.

~~~yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: the-deployment
spec:
  replicas: 3
~~~

If your organization also wanted to do canary deployments. The image could be changed within the *kustomize.yaml* file, an example can be found [here](https://github.com/kubernetes-sigs/kustomize/tree/master/examples/transformerconfigs#images-transformer).

Let's define the cluster1 application.

~~~sh
# Use the `argocd` binary to create the application
argocd app create --project default --name cluster1-web-app --repo http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git --path labs/lab-5-assets/overlay-assets/overlays/cluster1 --dest-server $(argocd cluster list | grep cluster1 | awk '{print $1}') --dest-namespace web-app --revision master --sync-policy automated
~~~

Verify the objects deployed correctly using the argocd binary.
~~~sh
# Use the `argocd` binary to sync the application
argocd app sync cluster1-web-app
# The helper command `wait-for-argo-app` will wait until the Application is healthy in Argo CD
wait-for-argo-app cluster1-web-app
# Use the `argocd` binary to check the status of `cluster1-web-app`
argocd app get cluster1-web-app
~~~

Now that `cluster1-web-app` application is healthy, we will define the two remaining clusters.

NOTE: The *--dest-server* parameter should refer to server value from `argocd cluster list`.
~~~sh
# Use the `argocd` binary to create the application in cluster2
argocd app create --project default --name cluster2-web-app --repo http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git --path labs/lab-5-assets/overlay-assets/overlays/cluster2 --dest-server $(argocd cluster list | grep cluster2 | awk '{print $1}') --dest-namespace web-app --revision master --sync-policy automated
# Use the `argocd` binary to create the application in cluster3
argocd app create --project default --name cluster3-web-app --repo http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git --path labs/lab-5-assets/overlay-assets/overlays/cluster3 --dest-server $(argocd cluster list | grep cluster3 | awk '{print $1}') --dest-namespace web-app --revision master --sync-policy automated
~~~

Verify the objects deployed correctly using the argocd binary.
~~~sh
# Use the `argocd` binary to sync the application
argocd app sync cluster2-web-app
# The helper command `wait-for-argo-app` will wait until the Application is healthy in Argo CD
wait-for-argo-app cluster2-web-app
# Display the status of the application
argocd app get cluster2-web-app
# Use the `argocd` binary to sync the application
argocd app sync cluster3-web-app
# The helper command `wait-for-argo-app` will wait until the Application is healthy in Argo CD
wait-for-argo-app cluster3-web-app
# Display the status of the application
argocd app get cluster3-web-app
~~~

Verify the deployments have been created on all the clusters.

~~~sh
# The for loop below will show the status of the deployment on the three clusters
for cluster in cluster1 cluster2 cluster3; do
    echo ------------ ${cluster} deployments ------------
    oc --context ${cluster} -n web-app get deployments
done
~~~

Finally, we will look up the OpenShift Route hostnames and verify that the unique values that were specified by the configmap are shown when we curl the page.

~~~sh
# The for loop below will get the `OpenShift Route` and then curl the application
for cluster in cluster1 cluster2 cluster3; do
  echo ------------ ${cluster} deployments ------------
  url=$(oc --context ${cluster} -n web-app get route the-route -o jsonpath='{.spec.host}')
  curl http://$url
done
~~~

Next Lab: [Lab 6 - Deploying MongoDB](./6.md)<br>
Previous Lab: [Lab 4 - Deploying and Managing a Project with GitOps](./4.md)<br>
[Home](./README.md)
