# Disaster Recovery

As we have seen, the strength of KubeFed is the ability to mange application workloads and move applications between clusters.

KubeFed primitives can be used to provide `Disaster Recovery` solutions for our applications.

In this lab we are going to see how KubeFed manages a failure of the primary MongoDB Replica.

## Creating some chaos

We are going to patch the `federateddeployment` for MongoDB application so we will scale the deployment to 0 in `cluster1` which is the cluster running the _PRIMARY_ ReplicaSet member.

~~~sh
oc --context=cluster1 -n mongo patch federateddeployment mongo --type=merge -p '{"spec":{"overrides":[{"clusterName":"cluster1","clusterOverrides":[{"path":"/spec/replicas","value":0}]}]}}'
~~~

The above command states that there should be 0 replicas in cluster1. To verify
no pods are running the following command can be ran.

> **NOTE:** It is possible that you see the pod in terminating status, that's fine.

~~~sh
oc --context=cluster1 -n mongo get deployment

NAME    READY   UP-TO-DATE   AVAILABLE   AGE
mongo   0/0     0            0           4m37s
~~~

Let's go ahead and remove the storage for MongoDB from `cluster1`:

~~~sh
oc --context=cluster1 -n mongo patch federatedpersistentvolumeclaims mongo --type=merge -p '{"spec":{"placement":{"clusters": [{"name":"cluster2"},{"name":"cluster3"}]}}}'
~~~

## Verifying Pacman Application still works

We have lost our primary MongoDB replica, but that didn't impact our application at all. Since we have three MongoDB replicas, the Pacman application can continue saving and reading high scores from the database.

You can go ahead and play Pacman, verify that high scores are saved.

## Bring the MongoDB replica back

Our engineers have been working hard during the weekend, we are ready to bring MongoDB replica on cluster1 back to life.

First we need to provision some storage:

~~~sh
oc --context=cluster1 -n mongo patch federatedpersistentvolumeclaims mongo --type=merge -p '{"spec":{"placement":{"clusters": [{"name":"cluster1"},{"name":"cluster2"},{"name":"cluster3"}]}}}'
~~~

And now deploy the replica by removing the overrides:

~~~sh
oc --context=cluster1 -n mongo patch federateddeployment mongo --type=merge -p '{"spec":{"overrides":[]}}'
~~~

We should see our MongoDB pod being created:

~~~sh 
oc --context=cluster1 -n mongo get pods

NAME                     READY   STATUS              RESTARTS   AGE
mongo-5f9ff55b67-5bgtc   0/1     ContainerCreating   0          8s
~~~

Once the pod is running the MongoDB replica will be reconfigured, we can get the new primary and secondary members by running:

~~~sh
wait-for-deployment cluster1 mongo mongo
MONGO_POD=$(oc --context=cluster1 -n mongo get pod --selector="name=mongo" --output=jsonpath='{.items..metadata.name}')
REPLICASET_STATUS=$(oc --context=cluster1 -n mongo exec $MONGO_POD -- bash -c 'mongo --norc --quiet --username=admin --password=$MONGODB_ADMIN_PASSWORD --host localhost admin --tls --tlsCAFile /opt/mongo-ssl/ca.pem --eval "JSON.stringify(rs.status())"')
# Get Primary Member
echo $REPLICASET_STATUS | jq '.members[] | select(.state | contains(1)).name'
# Get Secondary Members
echo $REPLICASET_STATUS | jq '.members[] | select(.state | contains(2)).name'
~~~

That concludes the disaster recovery lab.

Next Lab: [Lab 10 - Wrapup](./10.md)<br>
Previous Lab: [Lab 9 - Replica Scheduling](./9.md)<br>
[Home](./README.md)
