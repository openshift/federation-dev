# Working with Replica Scheduling Preference 
ReplicaSchedulingPreference provides an automated mechanism of distributing and maintaining total number of replicas for deployment or replicaset based federated workloads into federated clusters. This is based on high level user preferences given by the user. These preferences include the semantics of weighted distribution and limits (min and max) for distributing the replicas. These also include semantics to allow redistribution of replicas dynamically in case some replica pods remain unscheduled in some clusters, for example due to insufficient resources in that cluster.

We will be using the reverse-words federateddeployment from Lab 9.

The first schedule we will use is just a basic schedule to deploy 9 replicas across the three clusters.
~~~sh
cd ~/federation-dev/labs/scheduling-yaml
oc create -f basic-scheduling.yaml --context cluster1 -n reverse-words
~~~

Verify there are now 9 pods running between the three clusters.
~~~sh
for i in cluster1 cluster2 cluster3; do oc get deployment -n reverse-words --context $i; done
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   3/3     3            3           2d17h
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   3/3     3            3           2d17h
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   3/3     3            3           2d17h
~~~

We can also use weighted policies to ensure that certain clusters have higher scheduling priority for our application.

First, let's remove the previously used scheduler and remove any overrides created by the scheduler.
~~~sh
oc delete replicaschedulingpreference -n reverse-words reverse-words
oc -n reverse-words patch federateddeployment reverse-words -n reverse-words --type=merge -p '{"spec":{"overrides":[]}}'
~~~

Now create the weighted schedule.
~~~sh
oc apply -f weighted-scheduling.yaml --context cluster1 -n reverse-words
~~~

In the weighted-scheduling.yaml we added a weight to each cluster and increased the replica count as well. Let's verify the deployment has been modified as expected.
~~~sh
for i in cluster1 cluster2 cluster3; do oc get deployment -n reverse-words --context $i; done
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   3/3     3            3           2d17h
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   7/7     7            7           2d17h
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   10/10   10           10          2d17h
~~~ 

When using weights the minimum and maximum replicas can be specified as well. This will ensure that even though a cluster has a certain weight it does not overwhelm the particular cluster.

Delete the previously used weighted schedule.
~~~sh
oc delete replicaschedulingpreference -n reverse-words reverse-words
oc -n reverse-words patch federateddeployment reverse-words -n reverse-words --type=merge -p '{"spec":{"overrides":[]}}'
~~~

Apply the schedule specificing the minimum and maximum replicas.
~~~sh
oc apply -f min-max-scheduling.yaml --context cluster1 -n reverse-words
~~~

Verify the number of replicas falls between the minimum and maximum specifications and also that the pods are still deployed in a weighted order.
~~~sh
for i in cluster1 cluster2 cluster3; do oc get deployment -n reverse-words --context $i; done
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   6/6     6            6           2d17h
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   9/9     9            9           2d17h
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   5/5     5            5           2d17h
~~~ 

The same weighted value can be used on multiple clusers but a maximum replica count can be specified on a cluster. This could be useful in the event that a specific cluster has a lower amount of resources.

Remove the previously created minimum and maximums.
~~~sh
oc delete replicaschedulingpreference -n reverse-words reverse-words
oc -n reverse-words patch federateddeployment reverse-words -n reverse-words --type=merge -p '{"spec":{"overrides":[]}}'
~~~

Apply the *no-more-than-twenty.yaml* file.
~~~sh
oc apply -f no-more-than-twenty.yaml --context cluster1 -n reverse-words
~~~

Verify that the load is evenly distributed amongst clusters 1 and 3 but that cluster2 only has 20 replicas.
~~~sh
for i in cluster1 cluster2 cluster3; do oc get deployment -n reverse-words --context $i; done
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   30/30   30           30          2d17h
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   20/20   20           20          2d17h
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   30/30   30           30          2d17h
~~~ 

<!--
# Rebalancing
One of the strengths when using ReplicaSchedulingPreference is the ability to rebalance the workloads if a cluster goes offline. When the cluster goes offline 
the remaining replicas will be rescheduled to clusters that are currently in a Ready state.

Remove the previously used weighted scheduling preference.
~~~sh
oc delete replicaschedulingpreference -n reverse-words reverse-words
oc -n reverse-words patch federateddeployment reverse-words -n reverse-words --type=merge -p '{"spec":{"overrides":[]}}'
~~~

Apply the *rebalance.yaml* object. The object contains the value of *rebalance: true*.
~~~sh
oc apply -f rebalance.yaml  -n reverse-words
~~~

Verify that the replicas have been deployed and in the ready state.
~~~sh
for i in cluster1 cluster2 cluster3; do oc get deployment -n reverse-words --context $i; done
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   7/7     7            7           35m
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   6/6     6            6           32m
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
reverse-words   7/7     7            7           32m
~~~
-->

Next Lab: [Lab 10 - Wrap up ](./10.md)<br>
Previous Lab: [Lab 8 - Convert a Namespace to a Federated Namespace ](./8.md)<br>
[Home](../README.md)
