# Deploying Pacman

The files within the [directory](./lab-7-assets) are used by Argo CD to deploy
Pacman on multiple OpenShift clusters.

## Architecture

Below the architecture definition for our Pacman Application.

![Pacman Application Architecture](./assets/demo-arch.png)

* There is a Pacman pod running on each OpenShift Cluster
* There is an HAProxy Load Balancer which load balances the traffic on 
`pacman.example.com` across the three Pacman replicas
* Pacman saves highscores into MongoDB, the connection string to the database includes
all the three replicas hostnames

**How a user accesses the Pacman application**

1. The user points their browser to `pacman.example.com`, the DNS server will return the IP Address of the HAProxy
2. The browser will send a request to the HAProxy asking for the host `pacman.example.com`
3. The HAProxy will lookup the available backends servers for the frontend configuration `pacman.example.com`
4. The HAProxy will reverse proxy to one of the available backends servers (pacman pods) using Round Robin as load balance mechanism
5. The user will be connected to the backend server and presented with the Pacman app

## Prerequisites

### Deploying HAProxy

We are going to deploy the HAProxy server on one of the clusters (`cluster1`), the HAProxy could be hosted externally as well.

1. Change directory to `haproxy-yaml`

    ```sh
    cd ~/federation-dev/labs/haproxy-yaml
    ```
2. Create the namespace where the HAProxy LB will be deployed
    
    ```sh
    oc --context cluster1 create ns haproxy-lb
    ```
3. Create the HAProxy Route for external access

    > **NOTE:** HAPROXY_LB_ROUTE is what the diagram shows as `pacman.example.com`

    ```sh
    # Define the variable of `HAPROXY_LB_ROUTE`
    HAPROXY_LB_ROUTE=pacman-multicluster.$(oc --context=cluster1 get ingresses.config.openshift.io cluster -o jsonpath='{ .spec.domain }')
    # Use the value of `HAPROXY_LB_ROUTE` to create a route with the `oc` command
    oc --context cluster1 -n haproxy-lb create route edge haproxy-lb --service=haproxy-lb-service --port=8080 --insecure-policy=Allow --hostname=${HAPROXY_LB_ROUTE}
    ```
    > **NOTE:** The HAProxy Route will be used as the Pacman application entry point. Since we want to have HA across clusters, we will be access Pacman application using our HAProxy rather than OpenShift Routes
4. Create the configmap with the HAProxy configuration file

    ```sh
    # Define the variable of `PACMAN_INGRESS`
    PACMAN_INGRESS=pacman-ingress.$(oc --context=cluster1 get ingresses.config.openshift.io cluster -o jsonpath='{ .spec.domain }')
    # Define the variable of `PACMAN_CLUSTER1`
    PACMAN_CLUSTER1=pacman.$(oc --context=cluster1 get ingresses.config.openshift.io cluster -o jsonpath='{ .spec.domain }')
    # Define the variable of `PACMAN_CLUSTER2`
    PACMAN_CLUSTER2=pacman.$(oc --context=cluster2 get ingresses.config.openshift.io cluster -o jsonpath='{ .spec.domain }')
    # Define the variable of `PACMAN_CLUSTER3`
    PACMAN_CLUSTER3=pacman.$(oc --context=cluster3 get ingresses.config.openshift.io cluster -o jsonpath='{ .spec.domain }')
    # Copy the sample configmap
    cp haproxy.tmpl haproxy
    # Update the HAProxy configuration
    sed -i "/option httpchk GET/a \ \ \ \ http-request set-header Host ${PACMAN_INGRESS}" haproxy
    # Replace the value with the variable `PACMAN_INGRESS`
    sed -i "s/<pacman_lb_hostname>/${PACMAN_INGRESS}/g" haproxy
    # Replace the value with the variable `PACMAN_CLUSTER1`
    sed -i "s/<server1_name> <server1_pacman_route>:<route_port>/cluster1 ${PACMAN_CLUSTER1}:80/g" haproxy
    # Replace the value with the variable `PACMAN_CLUSTER2`
    sed -i "s/<server2_name> <server2_pacman_route>:<route_port>/cluster2 ${PACMAN_CLUSTER2}:80/g" haproxy
    # Replace the value with the variable `PACMAN_CLUSTER3`
    sed -i "s/<server3_name> <server3_pacman_route>:<route_port>/cluster3 ${PACMAN_CLUSTER3}:80/g" haproxy
    # Create the configmap
    oc --context cluster1 -n haproxy-lb create configmap haproxy --from-file=haproxy
    ```
    > **NOTE:** If you are curious about the HAProxy configuration, you can have a look at the `haproxy` file and review the frontend and backend sections
5. Create the HAProxy Service referenced in the HAProxy Route

    ```sh
    oc --context cluster1 -n haproxy-lb create -f haproxy-clusterip-service.yaml
    ```
6. Create the HAProxy Deployment

    ```sh
    oc --context cluster1 -n haproxy-lb create -f haproxy-deployment.yaml
    ```
7. Verify HAProxy is working

    7.1 Using the `wait-for-deploment` helper script check for HAProxy deployment to be ready

    ```sh
    wait-for-deployment cluster1 haproxy-lb haproxy-lb

    Checking if deployment haproxy-lb from namespace haproxy-lb on cluster cluster1 is ready
    <OUTPUT_OMITTED>
    Deployment is ready
    ```
    7.2 Try to access HAProxy

    > **NOTE:** 503 Service Unavailable means that no backend servers are available to handle HAProxy forwarded requests, but HAProxy is working fine.
    
    ```sh
    # Get the HAProxy LB Route
    HAPROXY_LB_ROUTE=$(oc --context cluster1 -n haproxy-lb get route haproxy-lb -o jsonpath='{.status.ingress[*].host}')
    # Access HAProxy
    curl -k https://${HAPROXY_LB_ROUTE}

    <html><body><h1>503 Service Unavailable</h1>
    No server is available to handle this request.
    </body></html>
    ```

## Deploying the Pacman Application

Now that the Mongo cluster has been configured, it is time to deploy the *pacman* application.

First, change to the directory of the Pacman demo folder.
~~~sh
cd ~/federation-dev/labs/lab-7-assets
~~~

For the *pacman* application, the file `pacman-deployment-rs.yaml` needs to reflect the MongoDB endpoint. The MongoDB endpoint is used to save scores from the game.
Provide the value of the MongoDB server(s) to be used for the scores to be recorded for the *pacman* game.

~~~sh
# Define the variable of `MONGO_CLUSTER1`
MONGO_CLUSTER1=$(oc --context=cluster1 -n mongo get route mongo -o jsonpath='{.status.ingress[*].host}')
# Define the variable of `MONGO_CLUSTER2`
MONGO_CLUSTER2=$(oc --context=cluster2 -n mongo get route mongo -o jsonpath='{.status.ingress[*].host}')
# Define the variable of `MONGO_CLUSTER3`
MONGO_CLUSTER3=$(oc --context=cluster3 -n mongo get route mongo -o jsonpath='{.status.ingress[*].host}')
# Use the variables to define the replica members
sed -i "s/replicamembershere/${MONGO_CLUSTER1},${MONGO_CLUSTER2},${MONGO_CLUSTER3}/g" base/pacman-deployment.yaml
~~~

A value must be provided to be the publicly accessible address for the *pacman* application.
~~~sh
# Define the variable of `PACMAN_INGRESS`
PACMAN_INGRESS=pacman-ingress.$(oc --context=cluster1 get ingresses.config.openshift.io cluster -o jsonpath='{ .spec.domain }')
# Replace the generic value with the variable `PACMAN_INGRESS`
sed -i "s/pacmanhosthere/${PACMAN_INGRESS}/g" base/pacman-route.yaml
~~~

1. Commit the files to the git repo

    ~~~sh
    # Stage your changes to be sent to the git repository
    git commit -am 'Values for MongoDB Replicas and Ingress Hostname'
    # Push your commits to the git repository
    git push origin master
    ~~~

2. We will deploy it on cluster1 and then run the same command against cluster2 and cluster3. We will use kustomize and overlays for creating the objects as we will be using the values to modify replica count and the image.

    ~~~sh
    # Use the `argocd` binary to create the application in cluster1
    argocd app create --project default --name cluster1-pacman --repo http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git --path labs/lab-7-assets/overlays/cluster1 --dest-server $(argocd cluster list | grep cluster1 | awk '{print $1}') --dest-namespace pacman --revision master --sync-policy automated
    ~~~

3. Now that the cluster1 has the application defined the other two clusters need to be defined as well.
    ~~~sh
    # Use the `argocd` binary to create the application in cluster2
    argocd app create --project default --name cluster2-pacman --repo http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git --path labs/lab-7-assets/overlays/cluster2 --dest-server $(argocd cluster list | grep cluster2 | awk '{print $1}') --dest-namespace pacman --revision master --sync-policy automated
    # Use the `argocd` binary to create the application in cluster3 
    argocd app create --project default --name cluster3-pacman --repo http://$(oc --context cluster1 -n gogs get route gogs -o jsonpath='{.spec.host}')/student/federation-dev.git --path labs/lab-7-assets/overlays/cluster3 --dest-server $(argocd cluster list | grep cluster3 | awk '{print $1}') --dest-namespace pacman --revision master --sync-policy automated
    ~~~

4. To help verify and speed up the deployment process we will use the sync command to verify that the items are deployed as expected.

    ~~~sh
    # The helper command `wait-for-argo-app` will wait until the Application is healthy in Argo CD
    wait-for-argo-app cluster1-pacman
    # Use the `argocd` binary sync the `cluster1-pacman`
    argocd app sync cluster1-pacman
    # The helper command `wait-for-argo-app` will wait until the Application is healthy in Argo CD
    wait-for-argo-app cluster2-pacman
    # Use the `argocd` binary to sync the `cluster2-pacman`
    argocd app sync cluster2-pacman
    # The helper command `wait-for-argo-app` will wait until the Application is healthy in Argo CD
    wait-for-argo-app cluster3-pacman
    # Use the `argocd` binary to sync the `cluster3-pacman`
    argocd app sync cluster3-pacman
    ~~~


5. Validate the namespace exists in the three clusters.
    ~~~sh
    # The for loop will verify the `namespace` is on the three clusters
    for i in cluster1 cluster2 cluster3; do oc get namespace pacman --context $i; done

    NAME    STATUS   AGE
    pacman   Active   8s
    NAME    STATUS   AGE
    pacman   Active   4s
    NAME    STATUS   AGE
    pacman   Active   6s
    ~~~
 

6. Wait for the deployment to become ready

    ~~~sh
    # The for loop uses the helper command `wait-for-deployment` to check for the Deployment object to be in the Ready state
    for cluster in cluster1 cluster2 cluster3; do wait-for-deployment $cluster pacman pacman;done

    Checking if deployment pacman from namespace pacman on cluster cluster1 is ready
    <OUTPUT_OMITTED>
    Deployment is ready
    Checking if deployment pacman from namespace pacman on cluster cluster2 is ready
    <OUTPUT_OMITTED>
    Deployment is ready
    Checking if deployment pacman from namespace pacman on cluster cluster3 is ready
    <OUTPUT_OMITTED>
    Deployment is ready
    ~~~

## Play the Game
The game should be available now at the publicly accessible address. Make sure to save the high score at the end of the game. This shows the data being persisted back to the database.

You can go ahead and open the url returned by the following command in your browser:

~~~sh
oc --context=cluster1 -n haproxy-lb get route haproxy-lb -o jsonpath="{.status.ingress[*].host}{\"\n\"}"

e.g: pacman-multicluster.apps.cluster-b5b7.b5b7.sandbox362.opentlc.com
~~~

If a mistake was made, please let an instructor know, and review the [cleanup instructions](./cleanup-instructions.md).

Next Lab: [Lab 8 - Application Portability ](./8.md)<br>
Previous Lab: [Lab 6 - Deploying MongoDB](./6.md)<br>
[Home](./README.md)
