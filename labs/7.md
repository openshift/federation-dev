# Deploying Pacman

The files within the [directory](./lab-7-assets) are used by Argo CD to deploy
Pacman on multiple OpenShift clusters.

## Architecture

Below the architecture definition for our Pacman Application.

![Pacman Application Architecture](./assets/demo-arch.png)

* There is a Pacman pod running on each OpenShift Cluster
* There is an HAProxy Load Balancer which load balances the traffic on 
`pacman.example.com` across the three Pacman replicas
* Pacman saves highscores into MongoDB, the connection string to the database includes
all the three replicas hostnames

**How a user accesses the Pacman application**

1. The user points their browser to `pacman.example.com`, the DNS server will return the IP Address of the HAProxy
2. The browser will send a request to the HAProxy asking for the host `pacman.example.com`
3. The HAProxy will lookup the available backends servers for the frontend configuration `pacman.example.com`
4. The HAProxy will reverse proxy to one of the available backends servers (pacman pods) using Round Robin as load balance mechanism
5. The user will be connected to the backend server and presented with the Pacman app

## Prerequisites

### Deploying HAProxy

We are going to deploy the HAProxy server on one of the clusters (`cluster1`), the HAProxy could be hosted externally as well.

1. Change directory to `haproxy-yaml`

    ```sh
    cd ~/federation-dev/labs/haproxy-yaml
    ```
2. Create the namespace where the HAProxy LB will be deployed
    
    ```sh
    oc --context cluster1 create ns haproxy-lb
    ```
3. Create the HAProxy Route for external access

    > **NOTE:** HAPROXY_LB_ROUTE is what the diagram shows as `pacman.example.com`

    ```sh
    HAPROXY_LB_ROUTE=pacman-multicluster.$(oc --context=cluster1 -n openshift-console get route console -o jsonpath='{.status.ingress[*].host}' | sed "s/.*\(apps.*\)/\1/g")
    oc --context cluster1 -n haproxy-lb create route edge haproxy-lb --service=haproxy-lb-service --port=8080 --insecure-policy=Allow --hostname=${HAPROXY_LB_ROUTE}
    ```
    > **NOTE:** The HAProxy Route will be used as the Pacman application entry point. Since we want to have HA across clusters, we will be access Pacman application using our HAProxy rather than OpenShift Routes
4. Create the configmap with the HAProxy configuration file

    ```sh
    # Export the required vars
    PACMAN_INGRESS=pacman-ingress.$(oc --context=cluster1 -n openshift-console get route console -o jsonpath='{.status.ingress[*].host}' | sed "s/.*\(apps.*\)/\1/g")
    PACMAN_CLUSTER1=pacman.$(oc --context=cluster1 -n openshift-console get route console -o jsonpath='{.status.ingress[*].host}' | sed "s/.*\(apps.*\)/\1/g")
    PACMAN_CLUSTER2=pacman.$(oc --context=cluster2 -n openshift-console get route console -o jsonpath='{.status.ingress[*].host}' | sed "s/.*\(apps.*\)/\1/g")
    PACMAN_CLUSTER3=pacman.$(oc --context=cluster3 -n openshift-console get route console -o jsonpath='{.status.ingress[*].host}' | sed "s/.*\(apps.*\)/\1/g")
    # Copy the sample configmap
    cp haproxy.tmpl haproxy
    # Update the HAProxy configuration
    sed -i "/option httpchk GET/a \ \ \ \ http-request set-header Host ${PACMAN_INGRESS}" haproxy
    sed -i "s/<pacman_lb_hostname>/${PACMAN_INGRESS}/g" haproxy
    sed -i "s/<server1_name> <server1_pacman_route>:<route_port>/cluster1 ${PACMAN_CLUSTER1}:80/g" haproxy
    sed -i "s/<server2_name> <server2_pacman_route>:<route_port>/cluster2 ${PACMAN_CLUSTER2}:80/g" haproxy
    sed -i "s/<server3_name> <server3_pacman_route>:<route_port>/cluster3 ${PACMAN_CLUSTER3}:80/g" haproxy
    # Create the configmap
    oc --context cluster1 -n haproxy-lb create configmap haproxy --from-file=haproxy
    ```
    > **NOTE:** If you are curious about the HAProxy configuration, you can have a look at the `haproxy` file and review the frontend and backend sections
5. Create the HAProxy Service referenced in the HAProxy Route

    ```sh
    oc --context cluster1 -n haproxy-lb create -f haproxy-clusterip-service.yaml
    ```
6. Create the HAProxy Deployment

    ```sh
    oc --context cluster1 -n haproxy-lb create -f haproxy-deployment.yaml
    ```
7. Verify HAProxy is working

    7.1 Wait for HAProxy deployment to be ready

    ```sh
    wait-for-deployment cluster1 haproxy-lb haproxy-lb

    Checking if deployment haproxy-lb from namespace haproxy-lb on cluster cluster1 is ready
    <OUTPUT_OMITTED>
    Deployment is ready
    ```
    7.2 Try to access HAProxy

    > **NOTE:** 503 Service Unavailable means that no backend servers are available to handle HAProxy forwarded requests, but HAProxy is working fine.
    
    ```sh
    # Get the HAProxy LB Route
    HAPROXY_LB_ROUTE=$(oc --context cluster1 -n haproxy-lb get route haproxy-lb -o jsonpath='{.status.ingress[*].host}')
    # Access HAProxy
    curl -k https://${HAPROXY_LB_ROUTE}

    <html><body><h1>503 Service Unavailable</h1>
    No server is available to handle this request.
    </body></html>
    ```

## Deploying the Pacman Application

Now that the Mongo cluster has been configured, it is time to deploy the *pacman* application.

First, change to the directory of the Pacman demo folder.
~~~sh
cd ~/federation-dev/labs/lab-7-assets/base
~~~

For the *pacman* application, the file `pacman-deployment-rs.yaml` needs to reflect the MongoDB endpoint. The MongoDB endpoint is used to save scores from the game.
Provide the value of the MongoDB server(s) to be used for the scores to be recorded for the *pacman* game.

~~~sh
MONGO_CLUSTER1=$(oc --context=cluster1 -n mongo get route mongo -o jsonpath='{.status.ingress[*].host}')
MONGO_CLUSTER2=$(oc --context=cluster2 -n mongo get route mongo -o jsonpath='{.status.ingress[*].host}')
MONGO_CLUSTER3=$(oc --context=cluster3 -n mongo get route mongo -o jsonpath='{.status.ingress[*].host}')
sed -i "s/replicamembershere/${MONGO_CLUSTER1},${MONGO_CLUSTER2},${MONGO_CLUSTER3}/g" 07-pacman-deployment-rs.yaml
~~~

A value must be provided to be the publicly accessible address for the *pacman* application.
~~~sh
PACMAN_INGRESS=pacman-ingress.$(oc --context=cluster1 -n openshift-console get route console -o jsonpath='{.status.ingress[*].host}' | sed "s/.*\(apps.*\)/\1/g")
sed -i "s/pacmanhosthere/${PACMAN_INGRESS}/g" 03-pacman-ingress.yaml
~~~

1. Commit the files to the git repo

    ~~~sh
    git commit -am 'values for mongo replicas and ingress'
    ~~~

2. Create the namespace in each cluster
   ~~~sh
   oc create ns pacman --context cluster1
   oc create ns pacman --context cluster2
   oc create ns pacman --context cluster3
   ~~~

3. We will deploy it on cluster1 and then run the same command against cluster2 and cluster3. We will use kustomize and overlays for creating the objects as we will be using the values to modify replica count and the image.

    ~~~sh
    argocd app create --project default --name cluster1-pacman --repo https://github.com/openshift/federation-dev.git --path labs/lab-7-assets --dest-server https://kubernetes.default.svc  --dest-namespace pacman  --revision master  --sync-policy automated
    ~~~

4. Now that the cluster1 has the application defined the other two clusters need to be defined as well.
    ~~~sh
    argocd app create --project default --name cluster2-pacman --repo https://github.com/openshift/federation-dev.git --path labs/lab-7-assets --dest-server https://api.east-2.sysdeseng.com:6443  --dest-namespace pacman  --revision master  --sync-policy automated
    argocd app create --project default --name cluster3-pacman --repo https://github.com/openshift/federation-dev.git --path labs/lab-7-assets --dest-server https://api.west-2.sysdeseng.com:6443  --dest-namespace pacman  --revision master  --sync-policy automated
    ~~~

5. Validate the namespace exists in the three clusters.
    ~~~sh
    for i in cluster1 cluster2 cluster3; do oc get namespace pacman --context $i; done

    NAME    STATUS   AGE
    pacman   Active   8s
    NAME    STATUS   AGE
    pacman   Active   4s
    NAME    STATUS   AGE
    pacman   Active   6s
    ~~~
 

6. Wait for the deployment to become ready

    ~~~sh
    for cluster in cluster1 cluster2 cluster3; do wait-for-deployment $cluster pacman pacman;done

    Checking if deployment pacman from namespace pacman on cluster cluster1 is ready
    <OUTPUT_OMITTED>
    Deployment is ready
    Checking if deployment pacman from namespace pacman on cluster cluster2 is ready
    <OUTPUT_OMITTED>
    Deployment is ready
    Checking if deployment pacman from namespace pacman on cluster cluster3 is ready
    <OUTPUT_OMITTED>
    Deployment is ready
    ~~~

## Play the Game
The game should be available now at the publicly accessible address. Make sure to save the high score at the end of the game. This shows the data being persisted back to the database.

You can go ahead and open the url returned by the following command in your browser:

~~~sh
oc --context=cluster1 -n haproxy-lb get route haproxy-lb -o jsonpath='{.status.ingress[*].host}'

e.g: pacman-multicluster.apps.cluster-b5b7.b5b7.sandbox362.opentlc.com
~~~

If a mistake was made, please let an instructor know, and review the [cleanup instructions](./cleanup-instructions.md).

Next Lab: [Lab 7 - Application Portability ](./7.md)<br>
Previous Lab: [Lab 5 - Deploying MongoDB](./5.md)<br>
[Home](./README.md)
