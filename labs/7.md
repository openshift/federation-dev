# Application Portability
The strength of KubeFed is the ability to mange application workloads and move applications between clusters.

We have two KubeFed primitives, Overrides and Placement, the former allow us to configure how applications are configured across our clusters, the later allow us to configure which clusters will run our application.

In this lab we are going to control where our application runs using overrides and placements. You will learn when to use what, and the differences between configuring placements or overrides to control where to run your applications.

## Moving the Application using Overrides
By patching the `federateddeployment` the application can be scheduled and unscheduled between the clusters. The step below scales to 0 the *pacman* application deployment from all clusters except for cluster2. This is done by modifying the `federateddeployment`
~~~sh
oc --context=cluster1 -n pacman patch federateddeployment pacman --type=merge -p '{"spec":{"overrides":[{"clusterName":"cluster3","clusterOverrides":[{"path":"/spec/replicas","value":0}]},{"clusterName":"cluster1","clusterOverrides":[{"path":"/spec/replicas","value":0}]}]}}'
~~~

The above command states that there should be 0 replicas in both cluster1 and cluster3. To verify
no pods are running in the other clusters the following command can be ran.

~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done

*** cluster1 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   0/0     0            0           6m21s
*** cluster2 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           6m21s
*** cluster3 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   0/0     0            0           6m21s
~~~

> **NOTE:** You can now go ahead and play Pacman, you should see that it doesn't matter how many times you reload the application, the request is going to be handled by Cluster 2 replica.

To populate the application back to all clusters remove the overrides created in the previous step.
~~~sh
oc --context=cluster1 -n pacman patch federateddeployment pacman --type=merge -p '{"spec":{"overrides":[]}}'
~~~

> **NOTE:** You can now go ahead and play Pacman, you should see that Pacman application requests are load balanced across all three clusters.

## Moving the Application using Placement policies

Another approach will be modifying the *pacman* application deployment placement. This is done by modifying the `federateddeployment`
~~~sh
oc --context=cluster1 -n pacman patch federateddeployment pacman --type=merge -p '{"spec":{"placement":{"clusters": [{"name":"cluster2"}]}}}'
~~~

The above commands states that the Pacman deployment should be present only in cluster2. To verify no deployments
are available in the other clusters the following command can be ran.

~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done

*** cluster1 ***
No resources found.
*** cluster2 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           8m23s
*** cluster3 ***
No resources found.
~~~

To populate the application back to all clusters, modify the placement to include cluster1 and cluster2.
~~~sh
oc --context=cluster1 -n pacman patch federateddeployment pacman --type=merge -p '{"spec":{"placement":{"clusters": [{"name":"cluster1"}, {"name":"cluster2"}, {"name":"cluster3"}]}}}'
~~~

Verify the deployments are created again in all three clusters.

~~~sh
for cluster in cluster1 cluster2 cluster3;do echo "*** $cluster ***"; oc get deployment --context $cluster -n pacman;done

*** cluster1 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           24s
*** cluster2 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           9m24s
*** cluster3 ***
NAME     READY   UP-TO-DATE   AVAILABLE   AGE
pacman   1/1     1            1           25s
~~~

Can you spot the difference between scaling a deployment to 0 in a cluster using `overrides` and removing a deployment from a cluster using `placement`?

The most important thing to note during the modification of which clusters are running the
*pacman* application is that the scores persist regardless of which cluster the application is running and HAProxy always ensures the application is available.

Next Lab: [Lab 8 - Convert a Namespace to a Federated Namespace](./8.md)<br>
Previous Lab: [Lab 6 - Deploying Pacman](./6.md)<br>
[Home](../README.md)
